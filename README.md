# Entityâ€‘Resolution Pipeline

This project implements a full pipeline for **entity resolution / deduplication / record linkage**, including blocking, matching (rules + machine learning), clustering, and canonicalization to create â€œgolden records.â€

## ğŸ§© Project Structure

```
Entityâ€‘resolution-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ clear_data.csv              # normalized input records
â”‚   â”œâ”€â”€ pair_model.joblib           # (optional) trained matching model
â”‚   â””â”€â”€ pair_model_meta.json        # metadata: features, threshold
â”‚
â”œâ”€â”€ out/
â”‚   â”œâ”€â”€ cand_pairs.csv              # candidate pairs after blocking
â”‚   â”œâ”€â”€ pairs_pred.csv              # predicted matching pairs
â”‚   â”œâ”€â”€ rows_with_entity_id.csv     # original rows annotated with entity IDs
â”‚   â””â”€â”€ entities.csv                # canonical (golden) records for each entity cluster
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipline.py                  # endâ€‘toâ€‘end pipeline (matching â†’ clustering â†’ canonicalization)
â”‚   â”œâ”€â”€ rules.py                    # normalization, feature extraction, rule logic & utilities
â”‚   â”œâ”€â”€ cluster.py                  # clustering logic (connected components, cluster metrics)
â”‚   â””â”€â”€ canonicalize.py             # canonicalization logic for merged entities
â”‚
â”œâ”€â”€ tests/                          # unit tests using pytest
â”œâ”€â”€ *.ipynb                         # notebooks: blocking, modeling, EDA, etc.
â”œâ”€â”€ README.txt                      # original README text file
â”œâ”€â”€ .gitignore
â””â”€â”€ requirements.txt
```

## ğŸš€ Installation

1. Clone the repository:

```bash
git clone https://github.com/SzaboIvan711/Entity-resolution-pipeline.git
cd Entity-resolution-pipeline
```

2. Create and activate a virtual environment:

```bash
python -m venv .venv
# On macOS / Linux:
source .venv/bin/activate
# On Windows:
.venv\Scripts\activate
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

## âš™ï¸ Usage

### Blocking â†’ Matching â†’ Clustering â†’ Canonicalization

1. Ensure you have **candidate pairs** after blocking (e.g. via `blocking.ipynb`) saved to `out/cand_pairs.csv`.  
2. Run the pipeline:

```bash
python -m src.pipline
# or
python src/pipline.py
```

3. The script will:
   - Load normalized records (`data/clear_data.csv`)
   - Load candidate pairs
   - If a trained model (`pair_model.joblib`) + metadata exist, use it; otherwise fallback to rule-based matching
   - Perform clustering and canonicalization
   - Save outputs into `out/`: `pairs_pred.csv`, `rows_with_entity_id.csv`, `entities.csv`

### Optional: Train / Update Matching Model

Use `model.ipynb` to train a new model, save it to `data/pair_model.joblib` and `data/pair_model_meta.json`.  

## âœ… Tests

Run tests with:

```bash
pytest -v
```

## ğŸ§  Concepts & Approach

- **Blocking**: reduce search space by heuristics (e.g., name prefix, zip code)
- **Matching**: rule-based or ML model
- **Clustering**: treat pairs as graph edges â†’ connected components
- **Canonicalization**: pick representative values (longest name, most frequent address, etc.)

## ğŸ“Œ Notes

- Works best with pre-cleaned data.
- Modular design â€” swap matching or clustering components easily.
- Tune model threshold to balance precision vs recall.

## ğŸ“ License

Add a license here (MIT, Apache 2.0, etc.)
