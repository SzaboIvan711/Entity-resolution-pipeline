{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training pairs and associated features for the model\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support\n",
    "from src.rules import pair_features, is_match, true_pairs, prepare_aux_cols\n",
    "\n",
    "df = pd.read_csv(\"data/clear_data.csv\", dtype={\"Phone_norm\": str, \"Zip_norm\": str})\n",
    "# df — normalized DataFrame with uid and *_norm columns\n",
    "df = prepare_aux_cols(df)  # just in case\n",
    "\n",
    "# cand_pairs: either already defined or load from file\n",
    "cand_df = pd.read_csv('out/cand_pairs.csv')\n",
    "cand_pairs = list(map(tuple, cand_df[['i','j']].to_numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b76e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315, 3, 0.9905660377358491)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_pairs_df(df, pairs):\n",
    "    rows = []\n",
    "    for i, j in pairs:\n",
    "        f = pair_features(df, i, j)                 # features\n",
    "        y = int(df.at[i, 'uid'] == df.at[j, 'uid']) # label: 1 if same uid, else 0\n",
    "        rows.append({**f, 'y': y, 'i': i, 'j': j,\n",
    "                     'uid_i': df.at[i, 'uid'], 'uid_j': df.at[j, 'uid']})\n",
    "    Xy = pd.DataFrame(rows)\n",
    "    for c in Xy.columns:             # convert bool → int\n",
    "        if Xy[c].dtype == bool:\n",
    "            Xy[c] = Xy[c].astype(int)\n",
    "    return Xy\n",
    "\n",
    "Xy = make_pairs_df(df, cand_pairs)\n",
    "\n",
    "# Balance: all positives + up to 3x negatives\n",
    "pos = Xy[Xy.y == 1]\n",
    "neg = Xy[Xy.y == 0].sample(n=min(len(Xy[Xy.y == 0]), len(pos)*3), random_state=42, replace=False)\n",
    "Xy_bal = pd.concat([pos, neg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "len(pos), len(neg), Xy_bal.y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = df['uid'].unique()\n",
    "rng = np.random.default_rng(42)\n",
    "test_uids = set(rng.choice(uids, size=max(1, int(0.2 * len(uids))), replace=False))\n",
    "\n",
    "# Split by unique uids: ensure that uids in the test set don't appear in training\n",
    "mask_test = Xy_bal.apply(lambda r: r.uid_i in test_uids and r.uid_j in test_uids, axis=1)\n",
    "train = Xy_bal.loc[~mask_test].copy()\n",
    "test  = Xy_bal.loc[mask_test].copy()\n",
    "\n",
    "# Basic feature set for the model\n",
    "feat_cols = ['name_sim', 'street_sim', 'zip_eq', 'city_eq', 'email_user_eq', 'phone_last4_eq']\n",
    "X_train, y_train = train[feat_cols], train['y']\n",
    "X_test,  y_test  = test[feat_cols],  test['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e2d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1 (GroupKFold): 0.9999999999995 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train machine learning model for entity matching\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "groups = train[['uid_i','uid_j']].max(axis=1)  # любая группировка по uid\n",
    "\n",
    "scores=[]\n",
    "for tr_idx, va_idx in gkf.split(train[feat_cols], train['y'], groups=groups):\n",
    "    clf_cv = LogisticRegression(max_iter=1000, class_weight='balanced').fit(\n",
    "        train.iloc[tr_idx][feat_cols], train.iloc[tr_idx]['y']\n",
    "    )\n",
    "    proba = clf_cv.predict_proba(train.iloc[va_idx][feat_cols])[:,1]\n",
    "    p,r,t = precision_recall_curve(train.iloc[va_idx]['y'], proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12); scores.append(f1[:-1].max())\n",
    "print('CV F1 (GroupKFold):', np.mean(scores), '±', np.std(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train y: {1: 227, 0: 23}\n",
      "test  y: {1: 88}\n"
     ]
    }
   ],
   "source": [
    "# 1) Helper: sample negative pairs (i, j) with different uids that are not already in the dataset\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def sample_random_negatives(df, n, banned_pairs, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = df.index.to_numpy()\n",
    "    out = set()\n",
    "    while len(out) < n:\n",
    "        i, j = map(int, rng.choice(idx, size=2, replace=False))\n",
    "        if df.at[i, 'uid'] != df.at[j, 'uid'] and (i, j) not in banned_pairs and (j, i) not in banned_pairs:\n",
    "            out.add((i, j))\n",
    "    return list(out)\n",
    "\n",
    "# 2) If TRAIN has no (or too few) negatives — add some, e.g., 20 samples\n",
    "need_neg_train = 20\n",
    "if (train['y'] == 0).sum() < need_neg_train:\n",
    "    banned_tr = set(map(tuple, train[['i', 'j']].to_numpy()))\n",
    "    extra_negs_tr = sample_random_negatives(df, n=need_neg_train, banned_pairs=banned_tr, seed=123)\n",
    "    Xy_negs_tr = make_pairs_df(df, extra_negs_tr)\n",
    "    train = pd.concat([train, Xy_negs_tr], ignore_index=True)\n",
    "\n",
    "print('train y:', train['y'].value_counts().to_dict())\n",
    "print('test  y:', test['y'].value_counts().to_dict())\n",
    "\n",
    "# 3) Rebuild feature matrices\n",
    "feat_cols = ['name_sim', 'street_sim', 'zip_eq', 'city_eq', 'email_user_eq', 'phone_last4_eq']\n",
    "X_train = train[feat_cols].copy();  X_train['street_sim'] /= 100.0\n",
    "y_train = train['y']\n",
    "\n",
    "X_test  = test[feat_cols].copy();   X_test['street_sim']  /= 100.0\n",
    "y_test  = test['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd421109",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def sample_random_negatives(df, n, banned):\n",
    "    \"\"\"Sample pairs (i, j) with different uids that are not in the banned set.\"\"\"\n",
    "    idx = df.index.to_numpy()\n",
    "    out = set()\n",
    "    while len(out) < n:\n",
    "        i, j = map(int, rng.choice(idx, size=2, replace=False))\n",
    "        if df.at[i, 'uid'] != df.at[j, 'uid'] and (i, j) not in banned and (j, i) not in banned:\n",
    "            out.add((i, j))\n",
    "    return list(out)\n",
    "\n",
    "# If the test set has only one class — add, for example, 20 negative pairs\n",
    "if y_test.nunique() < 2:\n",
    "    banned = set(map(tuple, test[['i', 'j']].to_numpy()))\n",
    "    extra_negs = sample_random_negatives(df, n=20, banned=banned)\n",
    "    Xy_negs = make_pairs_df(df, extra_negs)\n",
    "\n",
    "    test = pd.concat([test, Xy_negs], ignore_index=True)\n",
    "    X_test = test[feat_cols].copy()\n",
    "    X_test['street_sim'] /= 100.0\n",
    "    y_test = test['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367dfb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1] thr=0.929998\n",
      "[fold 2] thr=0.733296\n",
      "[fold 3] thr=0.804076\n",
      "[fold 4] thr=0.433905\n",
      "[fold 5] thr=0.333092\n",
      "best_thr (GroupKFold, median): 0.7332955615590503\n",
      "[[20  0]\n",
      " [ 0 88]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        20\n",
      "           1      1.000     1.000     1.000        88\n",
      "\n",
      "    accuracy                          1.000       108\n",
      "   macro avg      1.000     1.000     1.000       108\n",
      "weighted avg      1.000     1.000     1.000       108\n",
      "\n",
      "mean: 0.6468733837461482 std: 0.22634274877114427 iqr: 0.37017093472596124\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train machine learning model for entity matching\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# --- Quality hyperparameters ---\n",
    "MIN_PREC = 0.990   # we want very high precision (minimum false merges)\n",
    "MIN_REC  = 0.980   # and at the same time high recall\n",
    "\n",
    "def pick_threshold_by_constraints(y_true, proba, min_prec=MIN_PREC, min_rec=MIN_REC, beta_fallback=0.5):\n",
    "    \"\"\"Threshold selection: first find a point with p>=min_prec and r>=min_rec (take max F1);\n",
    "    if none exist, pick maximum F-beta (beta<1 penalizes false positives more strongly).\"\"\"\n",
    "    p, r, t = precision_recall_curve(y_true, proba)\n",
    "    p, r = p[:-1], r[:-1]   # align lengths\n",
    "    mask = (p >= min_prec) & (r >= min_rec)\n",
    "    if mask.any():\n",
    "        f1 = 2 * p * r / (p + r + 1e-12)\n",
    "        thr = float(t[mask][np.argmax(f1[mask])])\n",
    "    else:\n",
    "        beta = beta_fallback\n",
    "        fbeta = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)\n",
    "        thr = float(t[np.argmax(fbeta)])\n",
    "    return thr\n",
    "\n",
    "# --------- 1) Group validation on train ---------\n",
    "# train — your DataFrame with feat_cols + ['y', 'uid_i', 'uid_j']\n",
    "# X_train, y_train — already prepared earlier (scaling, feature selection)\n",
    "groups = train[['uid_i', 'uid_j']].max(axis=1)   # any deterministic grouping by uid\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "fold_thrs = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_train, y_train, groups=groups), 1):\n",
    "    clf_cv = LogisticRegression(max_iter=1000, class_weight='balanced').fit(\n",
    "        X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    )\n",
    "    proba_val = clf_cv.predict_proba(X_train.iloc[va_idx])[:, 1]\n",
    "    thr_fold = pick_threshold_by_constraints(y_train.iloc[va_idx], proba_val)\n",
    "    fold_thrs.append(thr_fold)\n",
    "    print(f'[fold {fold}] thr={thr_fold:.6f}')\n",
    "\n",
    "# Final threshold — median/mean across folds (robust)\n",
    "best_thr = float(np.median(fold_thrs))\n",
    "print('best_thr (GroupKFold, median):', best_thr)\n",
    "\n",
    "# --------- 2) Train final model on full train set ---------\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced').fit(X_train, y_train)\n",
    "\n",
    "# --------- 3) Apply to test ---------\n",
    "proba_test = clf.predict_proba(X_test)[:, 1]\n",
    "y_pred = (proba_test >= best_thr).astype(int)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels=[0, 1]))\n",
    "print(classification_report(y_test, y_pred, labels=[0, 1], zero_division=0, digits=3))\n",
    "\n",
    "print('mean:', np.mean(fold_thrs), 'std:', np.std(fold_thrs),\n",
    "      'iqr:', np.percentile(fold_thrs, 75) - np.percentile(fold_thrs, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7764e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid — P:0.991 R:1.000 F1:0.995   tp=315 fp=3 fn=0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance and compute ROC AUC or precision-recall metrics\n",
    "\n",
    "def model_score_pair(df, i, j, clf, feat_cols):\n",
    "    f = pair_features(df, i, j)\n",
    "    x = pd.DataFrame([[f.get(c, 0) for c in feat_cols]], columns=feat_cols)\n",
    "    return float(clf.predict_proba(x)[0, 1])\n",
    "\n",
    "def hybrid_is_match(df, i, j, clf, feat_cols, thr):\n",
    "    if is_match(df, i, j):                   # your rule-based baseline\n",
    "        return True\n",
    "    return model_score_pair(df, i, j, clf, feat_cols) >= thr\n",
    "\n",
    "# Predictions on all candidate pairs\n",
    "pred_pairs_hybrid = {(i, j) for i, j in cand_pairs if hybrid_is_match(df, i, j, clf, feat_cols, best_thr)}\n",
    "\n",
    "# Metrics\n",
    "T = true_pairs(df, uid_col='uid')\n",
    "tp = len(pred_pairs_hybrid & T)\n",
    "fp = len(pred_pairs_hybrid - T)\n",
    "fn = len(T - pred_pairs_hybrid)\n",
    "\n",
    "prec = tp / (tp + fp) if tp + fp else 0.0\n",
    "rec  = tp / (tp + fn) if tp + fn else 0.0\n",
    "f1   = 0 if prec + rec == 0 else 2 * prec * rec / (prec + rec)\n",
    "\n",
    "print(f'Hybrid — P:{prec:.3f} R:{rec:.3f} F1:{f1:.3f}   tp={tp} fp={fp} fn={fn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only_Model — P:0.991 R:1.000 F1:0.995   tp=315 fp=3 fn=0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance and compute ROC AUC or precision-recall metrics\n",
    "\n",
    "def model_score_pair(df, i, j, clf, feat_cols):\n",
    "    f = pair_features(df, i, j)\n",
    "    x = pd.DataFrame([[f.get(c, 0) for c in feat_cols]], columns=feat_cols)\n",
    "    return float(clf.predict_proba(x)[0, 1])\n",
    "\n",
    "def model_only_is_match(df, i, j, clf, feat_cols, thr):\n",
    "    # Removed rule-based baseline; use model-only prediction\n",
    "    return model_score_pair(df, i, j, clf, feat_cols) >= thr\n",
    "\n",
    "# Predictions on all candidate pairs\n",
    "pred_pairs_model = {(i, j) for i, j in cand_pairs if model_only_is_match(df, i, j, clf, feat_cols, best_thr)}\n",
    "\n",
    "# Metrics\n",
    "T = true_pairs(df, uid_col='uid')\n",
    "tp = len(pred_pairs_model & T)\n",
    "fp = len(pred_pairs_model - T)\n",
    "fn = len(T - pred_pairs_model)\n",
    "\n",
    "prec = tp / (tp + fp) if tp + fp else 0.0\n",
    "rec  = tp / (tp + fn) if tp + fn else 0.0\n",
    "f1   = 0 if prec + rec == 0 else 2 * prec * rec / (prec + rec)\n",
    "\n",
    "print(f'Only_Model — P:{prec:.3f} R:{rec:.3f} F1:{f1:.3f}   tp={tp} fp={fp} fn={fn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575de0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 250 test: 108\n",
      "pos/neg test: {1: 88, 0: 20}\n",
      "[[20  0]\n",
      " [ 0 88]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        20\n",
      "           1      1.000     1.000     1.000        88\n",
      "\n",
      "    accuracy                          1.000       108\n",
      "   macro avg      1.000     1.000     1.000       108\n",
      "weighted avg      1.000     1.000     1.000       108\n",
      "\n",
      "city_eq           1.965282\n",
      "street_sim        1.941397\n",
      "phone_last4_eq    1.928396\n",
      "zip_eq            1.801245\n",
      "name_sim          1.537217\n",
      "email_user_eq     1.438728\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Dataset size summary\n",
    "print('train:', len(train), 'test:', len(test))\n",
    "print('pos/neg test:', test['y'].value_counts().to_dict())\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Model coefficients (for future use — feature importance)\n",
    "coef = pd.Series(clf.coef_[0], index=feat_cols).sort_values(ascending=False)\n",
    "print(coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93455331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and metadata for later use\n",
    "\n",
    "import os, json, joblib\n",
    "os.makedirs('data', exist_ok=True)\n",
    "joblib.dump({'clf': clf, 'feat_cols': feat_cols, 'threshold': best_thr},\n",
    "            'data/pair_model.joblib')\n",
    "with open('data/pair_model_meta.json','w',encoding='utf-8') as f:\n",
    "    json.dump({'features': feat_cols, 'threshold': best_thr}, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
